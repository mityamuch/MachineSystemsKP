# Часть 1. Формулировка ML-задачи и выбор моделей

## 1. Бизнес-задача

Целью проекта является разработка системы динамического ценообразования для сервиса райдшеринга. Основная функция системы — прогнозировать дисбаланс спроса и предложения по геозонам города и корректировать тарифы в реальном времени для поддержания баланса рынка. Это позволяет снижать среднее время ожидания пассажиров, увеличивать долю обслуженных запросов (fill rate), оптимизировать загрузку водителей и стабильно повышать общую выручку платформы.

---

## 2. Формулировка задачи

### Тип задачи  
Проблема относится к классу задач регрессии. Модель должна прогнозировать будущие значения спроса и предложения на коротком горизонте (5–15 минут), а затем — итоговый дисбаланс между ними, который определяет величину динамического тарифного множителя (surge multiplier).

### Таргет  
Основной целевой переменной является:

- **demand_supply_gap(t+5)** — прогноз разницы между количеством запросов пассажиров (demand) и количеством доступных водителей (supply) в выбранной геозоне через 5 минут.

Альтернативно, прогноз спроса и предложения может быть выполнен отдельными моделями.

### Данные для обучения

Используются следующие группы признаков:

- **Исторические признаки:** количество запросов, количество доступных водителей, отмены, фактические уровни surge, время подачи и ожидания.
- **Пространственные признаки:** идентификатор зоны, плотность населения, связи с соседними зонами.
- **Временные признаки:** час дня, день недели, праздники, аномальные периоды.
- **Контекстные признаки:** погодные данные, наличие массовых мероприятий, трафик.
- **Streaming-признаки:** быстрые агрегаты за последние 1–5 минут — изменение спроса, отток или приток водителей, всплески запросов.
- **Системные признаки:** текущее ETA, коэффициент отмен, уровень конкуренции.

Эти данные позволяют моделировать как долгосрочные паттерны динамики рынка, так и краткосрочные колебания.

---

## 3. Выбор моделей

### 3.1. LightGBM (градиентный бустинг)

**Преимущества:**
- высокая скорость обучения и инференса;
- возможность дообучения (warm-start), что подходит для частых обновлений в реальном времени;
- устойчивость к шуму и разнородным табличным данным;
- простая интеграция в продакшен.

**Недостатки:**
- требуется ручное формирование lag-фичей и агрегатов;
- ограниченная способность моделировать долгосрочные временные зависимости;
- чувствительность к резким структурным изменениям рынка без частого переобучения.

LightGBM подходит как основная модель для оперативной коррекции динамической цены в реальном времени.

---

### 3.2. Transformer-архитектуры (DeepAR / Temporal Fusion Transformer)

**Преимущества:**
- способность моделировать сложные сезонные, недельные и суточные паттерны;
- эффективная работа с многомерными временными рядами;
- устойчивость к нерегулярным или сложным колебаниям спроса;
- возможность прогнозирования на горизонте 5–60 минут.

**Недостатки:**
- высокая вычислительная стоимость обучения;
- ограниченная частота переобучения (обычно 1–2 раза в сутки);
- сложная эксплуатация и необходимость GPU-ресурсов;
- меньшая гибкость в использовании сверхчастых окон (1–5 секунд).

Эти модели подходят для формирования долгосрочного и среднесрочного прогноза спроса и предложения.

---

## 4. Выбор комбинированной двухуровневой архитектуры

Для обеспечения высокой точности прогноза и способности адаптироваться к изменениям рынка в режиме реального времени применяется двухуровневая модель:

### Уровень 1: Модель долгосрочного прогноза (Transformer / DeepAR / TFT)

Задачи:
- прогнозирование спроса и предложения по зонам на горизонте 5–15 минут;
- моделирование недельных и суточных паттернов рынка;
- сглаживание шумов и пиков.

Входные данные:
- исторические временные ряды,
- погодные и календарные признаки,
- пространственные признаки зон,
- данные о событиях,
- агрегаты с частотой 5–15 минут.

Выходные данные:
- прогноз спроса,
- прогноз предложения,
- первичный показатель дисбаланса.

---

### Уровень 2: Модель оперативной коррекции (LightGBM / CatBoost)

Задачи:
- учёт данных в реальном времени (1–30 секунд),
- коррекция прогноза трансформера с учётом локальных всплесков,
- формирование окончательного значения дисбаланса,
- генерация surge-множителя.

Входные данные:
- прогнозы трансформер-модели,
- быстрые агрегаты спроса и предложения,
- текущая статистика отмен,
- ETA в зоне,
- приток/отток водителей,
- локальные контекстные факторы.

---

### Преимущества гибридной архитектуры

- **Высокая точность:** трансформеры обеспечивают точный прогноз трендов, бустинги корректируют его на основе актуального состояния рынка.
- **Быстрая реакция:** LightGBM позволяет изменять цены с задержкой < 100 мс.
- **Адаптивность:** GBDT может переобучаться каждые 30–60 минут.
- **Устойчивость:** трансформер сглаживает долгосрочный шум, а GBDT компенсирует локальные пики.
- **Гибкость:** модели независимы, легко масштабируются и обновляются.

---

Таким образом, оптимальным решением для задачи прогнозирования спроса, предложения и динамического ценообразования является комбинированная двухуровневая архитектура, объединяющая преимущества моделей семейства Transformer и градиентного бустинга (LightGBM).

---

# Часть 2. Проектирование архитектуры

## 2.1. Высокоуровневая архитектура системы

Общая архитектура системы динамического ценообразования включает следующие ключевые компоненты: источники данных (приложения пассажиров и водителей, внешние API), пайплайн обработки данных (потоковый и пакетный), хранилище признаков, ML-модели (Transformer + LightGBM), сервис инференса и сервис динамических цен.

![Высокоуровневая архитектура](images/1_high_level_architecture.drawio.png)

---

## 2.2. Архитектура пайплайна данных (Data Pipeline)

Пайплайн данных состоит из двух ветвей:
- **Потоковая обработка (Apache Flink)** — обработка событий в реальном времени с задержкой < 1 секунды, генерация online-признаков (текущий спрос, предложение, ETA, детекция всплесков).
- **Пакетная обработка (Apache Spark)** — построение исторических признаков (лаг-признаки, агрегаты, сезонность), запуск каждые 15 минут.

Данные поступают через Apache Kafka и сохраняются в двухуровневое хранилище признаков (Online — Redis, Offline — Delta Lake) и озеро данных (S3/HDFS).

![Архитектура пайплайна данных](images/2_data_pipeline.drawio.png)

---

## 2.3. Архитектура пайплайна обучения (Training Pipeline)

Пайплайн обучения включает:
- **Подготовка данных** — загрузка из озера данных и хранилища признаков, валидация, разбиение на train/validation/test.
- **Обучение моделей** — Transformer (1–2 раза в день, GPU) и LightGBM (каждые 30–60 минут, инкрементально).
- **Валидация** — проверка метрик (RMSE, MAE, MAPE < 15%).
- **Реестр моделей (MLflow)** — версионирование и управление жизненным циклом моделей.
- **Развёртывание** — канареечный деплой с автоматическим откатом при деградации.

![Архитектура пайплайна обучения](images/3_training_pipeline.drawio.png)

---

## 2.4. Архитектура пайплайна инференса (Inference Pipeline)

Инференс обеспечивает обработку запросов с пиковой нагрузкой 10 670 RPS и общей задержкой < 177 мс:
- **Transformer** — периодический batch-инференс каждые 5–15 минут, результаты кэшируются в Redis.
- **LightGBM** — real-time инференс с задержкой < 5 мс, использует online-признаки и прогнозы Transformer.
- **Сервис динамических цен** — применяет бизнес-правила (множитель 1,0x–5,0x, сглаживание, A/B тесты).

![Архитектура пайплайна инференса](images/4_inference_pipeline.drawio.png)

---

# Часть 3. Расчёты и нефункциональные требования

## 3.1. Исходные данные

| Параметр | Значение |
|----------|----------|
| Активные пользователи в день (DAU) | 2 432 132 |
| Пиковая нагрузка (RPS) | 10 670 запросов/сек |
| Максимальная задержка ответа (Latency) | 177 мс |
| Количество геозон | ~500 зон |
| Горизонт хранения данных | 90 дней (3 месяца) |

---

## 3.2. Расчёт требований к хранилищу

### 3.2.1. Online Feature Store (Redis)

**Назначение:** Хранение актуальных признаков для real-time инференса.

**Расчёт объёма данных на одну зону:**
- Количество признаков: ~50 (спрос, предложение, ETA, агрегаты и т.д.)
- Средний размер признака: 8 байт (float64)
- Размер записи на зону: 50 × 8 = **400 байт**
- С учётом метаданных и ключей: ~**600 байт/зону**

**Общий объём:**
- Количество зон: 500
- Хранение последних 5 временных точек (TTL 5 минут, обновление каждую минуту)
- Объём: 500 × 5 × 600 байт = **1.5 МБ**

**С учётом репликации (3 реплики) и запаса (×2):**
- **Итого: ~10 МБ** (минимальный Redis-кластер)

**Рекомендация:** Redis Cluster с 3 узлами по 1 ГБ RAM каждый (с большим запасом для пиков и роста).

---

### 3.2.2. Caching Layer (прогнозы Transformer)

**Назначение:** Кэширование прогнозов Transformer-модели.

**Расчёт:**
- Количество зон: 500
- Горизонт прогноза: 12 точек (каждые 5 минут на час вперёд)
- Размер прогноза на точку: 3 значения × 8 байт = 24 байта
- Размер прогноза на зону: 12 × 24 = **288 байт**
- Общий объём: 500 × 288 = **144 КБ**

**С репликацией и запасом:**
- **Итого: ~1 МБ** (размещается в том же Redis-кластере)

---

### 3.2.3. Offline Feature Store (Delta Lake / S3)

**Назначение:** Хранение исторических признаков для обучения моделей.

**Расчёт объёма данных:**

| Компонент | Расчёт | Объём |
|-----------|--------|-------|
| Событий в день | 2.4M DAU × 3 запроса/день в среднем | ~7.3M событий/день |
| Размер события | ~500 байт (JSON сжатый) | — |
| Сырые данные в день | 7.3M × 500 байт | **3.6 ГБ/день** |
| Сырые данные за 90 дней | 3.6 × 90 | **~330 ГБ** |

**Агрегированные признаки:**

| Компонент | Расчёт | Объём |
|-----------|--------|-------|
| Признаков на зону | 100 признаков × 8 байт | 800 байт |
| Временных точек в день | 24 × 60 / 15 = 96 (каждые 15 мин) | — |
| Объём на зону в день | 96 × 800 байт | ~77 КБ |
| Всего зон | 500 | — |
| Признаки в день | 500 × 77 КБ | **~38 МБ/день** |
| Признаки за 90 дней | 38 × 90 | **~3.5 ГБ** |

**Модели и артефакты:**
- Transformer-модель: ~500 МБ
- LightGBM-модель: ~50 МБ
- Версий моделей (10 последних): ~5.5 ГБ
- **Итого модели: ~6 ГБ**

**Общий объём Offline-хранилища:**

| Компонент | Объём |
|-----------|-------|
| Сырые данные (90 дней) | 330 ГБ |
| Агрегированные признаки | 3.5 ГБ |
| Модели и артефакты | 6 ГБ |
| Логи и метаданные | 10 ГБ |
| **Итого** | **~350 ГБ** |

**С учётом роста и запаса (×2):**
- **Рекомендуемый объём S3: 1 ТБ**

---

### 3.2.4. Data Lake (сырые события)

**Расчёт потока событий:**

| Источник | События/сек (пик) | Размер события | Поток |
|----------|-------------------|----------------|-------|
| Rider App (запросы) | ~3 000 | 300 байт | 0.9 МБ/с |
| Driver App (локации) | ~5 000 | 200 байт | 1.0 МБ/с |
| Driver App (статусы) | ~1 000 | 150 байт | 0.15 МБ/с |
| Внешние API (погода, трафик) | ~100 | 500 байт | 0.05 МБ/с |
| **Итого (пик)** | **~9 100** | — | **~2.1 МБ/с** |

**Суточный объём:**
- Средняя нагрузка: ~50% от пиковой = 1 МБ/с
- В сутки: 1 МБ/с × 86 400 сек = **~86 ГБ/день**
- За 90 дней: **~7.7 ТБ**

**С учётом сжатия Parquet (×0.3) и запаса:**
- **Рекомендуемый объём Data Lake: 3 ТБ**

---

### 3.2.5. Сводная таблица требований к хранилищу

| Компонент | Тип хранилища | Объём | Примечание |
|-----------|---------------|-------|------------|
| Online Feature Store | Redis Cluster | 3 ГБ RAM | 3 узла × 1 ГБ |
| Caching Layer | Redis | (входит в Online FS) | — |
| Offline Feature Store | Delta Lake (S3) | 1 ТБ | SSD для быстрого доступа |
| Data Lake | S3 / HDFS | 3 ТБ | HDD, холодное хранение |
| Model Registry | S3 | 50 ГБ | Артефакты MLflow |
| Логи и мониторинг | Elasticsearch | 500 ГБ | 30 дней ротации |
| **Итого** | — | **~5 ТБ** | — |

---

## 3.3. Расчёт требований к пропускной способности (Throughput)

### 3.3.1. Inference Pipeline

**Требования:**
- Пиковая нагрузка: **10 670 RPS**
- Максимальная задержка: **177 мс (P99)**

**Разбивка задержки по компонентам:**

| Компонент | Задержка (P99) |
|-----------|----------------|
| Сетевой вызов (клиент → API Gateway) | 20 мс |
| API Gateway → Surge Service | 5 мс |
| Запрос к Online Feature Store (Redis) | 5 мс |
| Запрос к Caching Layer (Redis) | 5 мс |
| Инференс LightGBM | 5 мс |
| Бизнес-логика Surge Service | 10 мс |
| Ответ клиенту | 20 мс |
| **Итого** | **70 мс** |
| **Запас** | **107 мс** |

**Вывод:** Бюджет задержки выполняется с запасом ~60%.

---

### 3.3.2. Расчёт количества реплик сервиса инференса

**Производительность одного пода:**
- LightGBM инференс: ~5 мс
- Накладные расходы: ~15 мс
- Время обработки запроса: ~20 мс
- Throughput одного пода: 1000 мс / 20 мс = **50 RPS/под**

**Расчёт количества подов:**
- Пиковая нагрузка: 10 670 RPS
- Требуемых подов: 10 670 / 50 = **214 подов**
- С учётом запаса 30%: **~280 подов**

**Рекомендация:** Kubernetes Deployment с HPA (Horizontal Pod Autoscaler):
- Min replicas: 50 (базовая нагрузка)
- Max replicas: 300 (пиковая нагрузка)
- Target CPU: 70%

---

### 3.3.3. Streaming Pipeline (Apache Flink)

**Входящий поток:**
- События: ~9 100 событий/сек (пик)
- Размер: ~2.1 МБ/с

**Требования к Flink-кластеру:**
- TaskManagers: 4 (по 4 CPU, 8 ГБ RAM каждый)
- Parallelism: 16
- Checkpointing: каждые 30 сек

**Пропускная способность:**
- Один TaskManager: ~3 000 событий/сек
- Кластер: 4 × 3 000 = **12 000 событий/сек** (запас 30%)

---

### 3.3.4. Batch Pipeline (Apache Spark)

**Требования:**
- Обработка 15-минутного окна данных
- Время выполнения: < 10 минут (чтобы успеть до следующего запуска)

**Объём данных за 15 минут:**
- События: 9 100 × 900 сек × 0.5 (средняя нагрузка) = ~4M событий
- Размер: ~2 ГБ

**Конфигурация Spark-кластера:**
- Driver: 4 CPU, 8 ГБ RAM
- Executors: 8 × (4 CPU, 8 ГБ RAM)
- Время обработки: ~5 минут (запас 50%)

---

### 3.3.5. Kafka

**Требования:**
- Входящий поток: ~9 100 событий/сек
- Размер сообщения: ~300 байт (среднее)
- Throughput: ~2.7 МБ/с

**Конфигурация:**
- Брокеров: 3
- Партиций на топик: 12 (по числу зон / 50)
- Replication factor: 3
- Retention: 7 дней

**Пропускная способность кластера:**
- Один брокер: ~50 МБ/с
- Кластер: 150 МБ/с (запас ×50)

---

## 3.4. Масштабируемость и надёжность

### 3.4.1. Стратегия масштабирования

| Компонент | Тип масштабирования | Механизм |
|-----------|---------------------|----------|
| Inference Service | Горизонтальное | Kubernetes HPA по CPU/RPS |
| Redis Cluster | Горизонтальное | Добавление шардов |
| Kafka | Горизонтальное | Добавление брокеров и партиций |
| Flink | Горизонтальное | Добавление TaskManagers |
| Spark | Горизонтальное | Динамическое выделение executors |
| Transformer Serving | Горизонтальное | Kubernetes + GPU nodes |

---

### 3.4.2. Отказоустойчивость

**Уровни резервирования:**

| Компонент | SLA | Стратегия |
|-----------|-----|-----------|
| Redis Cluster | 99.99% | 3 реплики, automatic failover |
| Kafka | 99.99% | 3 реплики, ISR (In-Sync Replicas) |
| Inference Service | 99.9% | Multi-AZ deployment, 3+ реплик |
| API Gateway | 99.99% | Load Balancer + health checks |
| Data Lake (S3) | 99.999999999% | Built-in redundancy |

**Механизмы отказоустойчивости:**

1. **Circuit Breaker** — при недоступности Redis или ML-сервиса возвращается базовый множитель (1.0x).

2. **Graceful Degradation:**
   - При сбое Transformer → используются последние кэшированные прогнозы
   - При сбое LightGBM → используются прогнозы Transformer напрямую
   - При сбое Feature Store → используются default-значения признаков

3. **Health Checks:**
   - Liveness probe: каждые 10 сек
   - Readiness probe: каждые 5 сек

4. **Автоматический откат модели:**
   - При деградации метрик > 10% → откат к предыдущей версии

---

### 3.4.3. Disaster Recovery

| Метрика | Целевое значение |
|---------|------------------|
| RTO (Recovery Time Objective) | < 15 минут |
| RPO (Recovery Point Objective) | < 5 минут |

**Стратегия:**
- Multi-AZ deployment в основном регионе
- Резервный регион (warm standby) с репликацией данных
- Автоматическое переключение через Route 53 / Global Load Balancer

---

### 3.4.4. Мониторинг и алертинг

**Ключевые метрики:**

| Категория | Метрики | Порог алерта |
|-----------|---------|--------------|
| Latency | P50, P95, P99 | P99 > 150 мс |
| Throughput | RPS, успешных/неуспешных | Error rate > 1% |
| Ресурсы | CPU, RAM, Disk | CPU > 80% |
| ML-качество | MAPE, drift score | MAPE > 20% |
| Бизнес | Fill rate, avg surge | Anomaly detection |

**Стек мониторинга:**
- Метрики: Prometheus + Grafana
- Логи: ELK Stack (Elasticsearch, Logstash, Kibana)
- Трейсинг: Jaeger
- Алертинг: PagerDuty

---

## 3.5. Сводка нефункциональных требований

| Требование | Целевое значение | Обеспечение |
|------------|------------------|-------------|
| Latency (P99) | < 177 мс | ~70 мс (запас 60%) |
| Throughput | 10 670 RPS | До 15 000 RPS (запас 40%) |
| Availability | 99.9% | Multi-AZ, репликация |
| Хранилище | 5 ТБ | S3 + Redis Cluster |
| Масштабируемость | ×3 от текущей нагрузки | HPA, горизонтальное масштабирование |
| Recovery Time | < 15 мин | Warm standby, автофейловер |

---
