# Часть 1. Формулировка ML-задачи и выбор моделей

## 1. Бизнес-задача

Целью проекта является разработка системы динамического ценообразования для сервиса райдшеринга. Основная функция системы — прогнозировать дисбаланс спроса и предложения по геозонам города и корректировать тарифы в реальном времени для поддержания баланса рынка. Это позволяет снижать среднее время ожидания пассажиров, увеличивать долю обслуженных запросов (fill rate), оптимизировать загрузку водителей и стабильно повышать общую выручку платформы.

---

## 2. Формулировка задачи

### Тип задачи  
Проблема относится к классу задач регрессии. Модель должна прогнозировать будущие значения спроса и предложения на коротком горизонте (5–15 минут), а затем — итоговый дисбаланс между ними, который определяет величину динамического тарифного множителя (surge multiplier).

### Таргет  
Основной целевой переменной является:

- **demand_supply_gap(t+5)** — прогноз разницы между количеством запросов пассажиров (demand) и количеством доступных водителей (supply) в выбранной геозоне через 5 минут.

Альтернативно, прогноз спроса и предложения может быть выполнен отдельными моделями.

### Данные для обучения

Используются следующие группы признаков:

- **Исторические признаки:** количество запросов, количество доступных водителей, отмены, фактические уровни surge, время подачи и ожидания.
- **Пространственные признаки:** идентификатор зоны, плотность населения, связи с соседними зонами.
- **Временные признаки:** час дня, день недели, праздники, аномальные периоды.
- **Контекстные признаки:** погодные данные, наличие массовых мероприятий, трафик.
- **Streaming-признаки:** быстрые агрегаты за последние 1–5 минут — изменение спроса, отток или приток водителей, всплески запросов.
- **Системные признаки:** текущее ETA, коэффициент отмен, уровень конкуренции.

Эти данные позволяют моделировать как долгосрочные паттерны динамики рынка, так и краткосрочные колебания.

---

## 3. Выбор моделей

### 3.1. LightGBM (градиентный бустинг)

**Преимущества:**
- высокая скорость обучения и инференса;
- возможность дообучения (warm-start), что подходит для частых обновлений в реальном времени;
- устойчивость к шуму и разнородным табличным данным;
- простая интеграция в продакшен.

**Недостатки:**
- требуется ручное формирование lag-фичей и агрегатов;
- ограниченная способность моделировать долгосрочные временные зависимости;
- чувствительность к резким структурным изменениям рынка без частого переобучения.

LightGBM подходит как основная модель для оперативной коррекции динамической цены в реальном времени.

---

### 3.2. Transformer-архитектуры (DeepAR / Temporal Fusion Transformer)

**Преимущества:**
- способность моделировать сложные сезонные, недельные и суточные паттерны;
- эффективная работа с многомерными временными рядами;
- устойчивость к нерегулярным или сложным колебаниям спроса;
- возможность прогнозирования на горизонте 5–60 минут.

**Недостатки:**
- высокая вычислительная стоимость обучения;
- ограниченная частота переобучения (обычно 1–2 раза в сутки);
- сложная эксплуатация и необходимость GPU-ресурсов;
- меньшая гибкость в использовании сверхчастых окон (1–5 секунд).

Эти модели подходят для формирования долгосрочного и среднесрочного прогноза спроса и предложения.

---

## 4. Выбор комбинированной двухуровневой архитектуры

Для обеспечения высокой точности прогноза и способности адаптироваться к изменениям рынка в режиме реального времени применяется двухуровневая модель:

### Уровень 1: Модель долгосрочного прогноза (Transformer / DeepAR / TFT)

Задачи:
- прогнозирование спроса и предложения по зонам на горизонте 5–15 минут;
- моделирование недельных и суточных паттернов рынка;
- сглаживание шумов и пиков.

Входные данные:
- исторические временные ряды,
- погодные и календарные признаки,
- пространственные признаки зон,
- данные о событиях,
- агрегаты с частотой 5–15 минут.

Выходные данные:
- прогноз спроса,
- прогноз предложения,
- первичный показатель дисбаланса.

---

### Уровень 2: Модель оперативной коррекции (LightGBM / CatBoost)

Задачи:
- учёт данных в реальном времени (1–30 секунд),
- коррекция прогноза трансформера с учётом локальных всплесков,
- формирование окончательного значения дисбаланса,
- генерация surge-множителя.

Входные данные:
- прогнозы трансформер-модели,
- быстрые агрегаты спроса и предложения,
- текущая статистика отмен,
- ETA в зоне,
- приток/отток водителей,
- локальные контекстные факторы.

---

### Преимущества гибридной архитектуры

- **Высокая точность:** трансформеры обеспечивают точный прогноз трендов, бустинги корректируют его на основе актуального состояния рынка.
- **Быстрая реакция:** LightGBM позволяет изменять цены с задержкой < 100 мс.
- **Адаптивность:** GBDT может переобучаться каждые 30–60 минут.
- **Устойчивость:** трансформер сглаживает долгосрочный шум, а GBDT компенсирует локальные пики.
- **Гибкость:** модели независимы, легко масштабируются и обновляются.

---

Таким образом, оптимальным решением для задачи прогнозирования спроса, предложения и динамического ценообразования является комбинированная двухуровневая архитектура, объединяющая преимущества моделей семейства Transformer и градиентного бустинга (LightGBM).

---

# Часть 2. Проектирование архитектуры

### 1.1. Источники данных

**Клиентские приложения:**
- **Rider App** — генерирует события: запросы поездок, отмены, координаты, время ожидания.
- **Driver App** — генерирует события: статус онлайн/офлайн, завершение поездок, координаты, принятие/отклонение заказов.

**Внешние сервисы:**
- **External APIs** — погодные данные (OpenWeatherMap, WeatherAPI), данные о трафике (Google Maps API, TomTom), информация о массовых мероприятиях.

**Внутренние сервисы:**
- **Routing Service** — расчёт маршрутов и ETA (Estimated Time of Arrival).
- **Billing Service** — история транзакций, тарифы, скидки.
- **Geofencing Service** — определение геозон, границы районов.

**Все события** поступают в **Apache Kafka** — распределённую очередь сообщений, обеспечивающую надёжную доставку и горизонтальное масштабирование.

---

### 1.2. Data Pipeline

#### 1.2.1. Streaming Pipeline (Real-Time Processing)

**Технологии:** Apache Flink / Apache Spark Streaming

**Функции:**
- Обработка событий из Kafka в реальном времени (latency < 1 секунда).
- Генерация online features:
  - Текущий спрос (количество активных запросов за последние 1–5 минут).
  - Текущее предложение (количество доступных водителей в зоне).
  - Коэффициент отмен (cancel rate) за последние 5 минут.
  - Средний ETA в зоне.
  - Всплески запросов (spike detection).
  - Приток/отток водителей (driver churn rate).
  - Плотность запросов по подзонам.

**Выходные данные:**
- **Online Feature Store** (Redis Cluster) — для real-time инференса, обновление каждые 1–30 секунд.
- **Data Lake** (S3 / HDFS) — для последующей обработки в batch-пайплайне и обучения моделей, хранение в parquet.

#### 1.2.2. Batch Pipeline (Offline Processing)

**Технологии:** Apache Spark + Apache Airflow

**Функции:**
- Построение offline features:
  - Исторические лаги (lag features) — спрос/предложение на 1, 5, 15, 30, 60 минут назад.
  - Агрегаты по временным окнам — средние, медианы, стандартные отклонения за час, день, неделю.
  - Сезонные паттерны — день недели, час дня, праздники.
  - Пространственные агрегаты — связи между соседними зонами, миграция спроса.
  - Тренды и скользящие средние.
  - Статистики по зонам — исторические surge-множители, fill rate, среднее время ожидания.

**Расписание:** Apache Airflow запускает агрегацию каждые 15 минут для обновления признаков за последний период.

**Выходные данные:**
- **Offline Feature Store** (Offline Feature Store) — для обучения моделей и batch-инференса.

---

### 1.3. Feature Store

**Двухуровневая архитектура Feature Store:**

#### Online Feature Store (Redis Cluster)
- **Назначение:** Хранение актуальных признаков для real-time инференса.
- **Обновление:** Каждые 1–30 секунд из Streaming Pipeline.
- **Структура данных:** Key-value хранилище, ключ — `{zone_id}:{timestamp}`, значение — JSON с признаками.
- **TTL:** 1–5 минут для актуальных данных.
- **Производительность:** Latency < 5 мс на чтение.

#### Offline Feature Store (Delta Lake)
- **Назначение:** Хранение исторических и тяжёлых признаков для обучения.
- **Обновление:** Каждые 15 минут из Batch Pipeline.
- **Структура данных:** Таблицы с партиционированием по дате и зоне.
- **Согласованность:** Обеспечивает идентичные признаки для обучения и инференса через единую схему.

---

### 1.4. Training Pipeline

**Технологии:** MLflow, Kubeflow Pipelines, Apache Airflow

**Компоненты:**

#### 1.4.1. Data Preparation
- Загрузка данных из Data Lake и Offline Feature Store.
- Валидация данных (проверка на пропуски, аномалии, дрейф).
- Разделение на train/validation/test с учётом временного порядка.

#### 1.4.2. Model Training

**Transformer Model (Long-Term Forecast):**
- **Частота обучения:** 1–2 раза в сутки (ночью, когда нагрузка минимальна).
- **Горизонт прогноза:** 5–60 минут.
- **Входные данные:** Исторические временные ряды (спрос, предложение), календарные признаки, погодные данные, пространственные признаки.
- **Выходные данные:** Прогноз спроса, прогноз предложения, первичный дисбаланс по зонам.

**LightGBM Model (Short-Term Correction):**
- **Частота обучения:** Каждые 30–60 минут (incremental learning / warm-start).
- **Горизонт прогноза:** 1–5 минут.
- **Входные данные:** Прогнозы трансформера, online features, текущие метрики.
- **Выходные данные:** Скорректированный дисбаланс, ценовой-множитель.

#### 1.4.3. Model Validation & Registry
- **Валидация:** Метрики (RMSE, MAE, MAPE) на validation-данных.
- **Model Registry (MLflow):** Версионирование моделей, метаданные, артефакты.

---

### 1.5. Inference Pipeline

#### 1.5.1. Long-Term Predictor (Transformer Model)

**Технологии:** TensorFlow Serving / TorchServe

**Функции:**
- Периодический batch-инференс (каждые 5–15 минут) для всех зон.
- Прогнозирование спроса и предложения на горизонте 5–60 минут.
- Генерация первичного показателя дисбаланса.

**Результаты:**
- Записываются в **Caching Layer** (Redis Cluster) с TTL 15 минут.

**Масштабирование:** Горизонтальное масштабирование через Kubernetes.

#### 1.5.2. Real-Time Predictor (LightGBM Model)

**Технологии:** MLflow Models / собственный serving

**Функции:**
- Вызывается на каждый запрос динамической цены (10,670 RPS пиковая нагрузка).
- Забирает данные:
  - Online features из Online Feature Store (Redis).
  - Прогнозы трансформера из Caching Layer.
  - Контекстные признаки (текущее время, погода, события).

**Процесс:**
1. Получение запроса от Surge Pricing Service.
2. Загрузка признаков из Online FS и кэша (< 10 мс).
3. Инференс LightGBM (< 5 мс).
4. Возврат результата (surge-множитель, дисбаланс).

**Требования к производительности:**
- Latency < 50 мс (включая загрузку признаков).

---

### 1.6. Surge Pricing Service

**Технологии:** Микросервис

**Функции:**
- Получение результатов от Real-Time Predictor.
- Применение бизнес-правил:
  - Ограничения на surge-множитель (мин. 1.0x, макс. 5.0x).
  - Сглаживание резких скачков (rate limiting).
  - Учёт региональных особенностей.
  - Интеграция с A/B тестами.
- Формирование итоговой цены для пассажира и водителя.

**Выходные данные:**
- Возврат цены в **Rider App** и **Driver App** через API Gateway.
- Логирование всех решений для мониторинга и обратной связи.

**Latency:** < 100 мс (включая вызов ML-модели).

---

### 1.7. Инфраструктура

- **Оркестрация:** Kubernetes — управление контейнерами, автоскейлинг, отказоустойчивость.
- **Кэширование:** Redis Cluster — кэширование прогнозов и признаков.
- **Хранилище:** S3 / HDFS — Data Lake для долгосрочного хранения.
- **Базы данных:** PostgreSQL — метаданные моделей, конфигурации; Redis — online features.

---

### Итог

1. **Сбор данных:** События из Rider/Driver App и внешних сервисов → Kafka.
2. **Streaming обработка:** Kafka → Flink/Spark Streaming → Online Feature Store + Data Lake.
3. **Batch обработка:** Data Lake → Spark → Offline Feature Store.
4. **Обучение:** Offline FS + Data Lake → Training Pipeline → Model Registry.
5. **Инференс:**
   - Transformer: Периодический batch → Caching Layer.
   - LightGBM: Запрос → Online FS + Caching Layer → Surge Pricing Service → Rider/Driver App.

---
